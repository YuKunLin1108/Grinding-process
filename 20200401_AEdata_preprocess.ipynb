{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal \n",
    "import scipy.fftpack\n",
    "from pylab import *\n",
    "from scipy import pi\n",
    "import pylab\n",
    "import cmath\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "#import lvm_read\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all using files path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get want using file path\n",
    "import os\n",
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "def data_path(mypath):\n",
    "    filepath = []\n",
    "    ae_path = []\n",
    "    cu_path = []\n",
    "    ae_path_container = []\n",
    "    cu_path_container = []\n",
    "    # 遞迴列出所有檔案的絕對路徑\n",
    "    for root, dirs, files in walk(mypath):\n",
    "        for f in files:\n",
    "            if f.find(\"AEdate\") >= 0: ae_path.append(f)\n",
    "            elif f.find(\"Current\") >= 0: cu_path.append(f)\n",
    "#             elif f.find(\".csv\") >=0: equ_path = f\n",
    "        ae_path.sort(key = lambda x: int(x[7:-4]))\n",
    "        cu_path.sort(key = lambda x: int(x[8:-4]))\n",
    "        for class_ in ae_path:\n",
    "            files_path = os.path.join(root, class_)           \n",
    "            ae_path_container.append(files_path)\n",
    "        for class_ in cu_path:\n",
    "            files_path = os.path.join(root, class_)           \n",
    "            cu_path_container.append(files_path)\n",
    "#         equ_path_container = equ_path\n",
    "    return ae_path_container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dryrun_1_data : (57,)\n",
      "dryrun_2_data : (68,)\n",
      "dryrun_3_data : (306,)\n",
      "row0001_1_data : (180,)\n",
      "row0001_2_data : (174,)\n",
      "row0001_3_data : (168,)\n",
      "row005_1_data : (200,)\n",
      "row005_2_data : (201,)\n",
      "row005_3_data : (101,)\n",
      "row01_1_data : (200,)\n",
      "row01_2_data : (200,)\n",
      "row01_3_data : (152,)\n",
      "row1_1_data : (200,)\n",
      "row1_2_data : (201,)\n",
      "row1_3_data : (200,)\n",
      "row3_1_data : (202,)\n",
      "row3_2_data : (200,)\n",
      "row3_3_data : (201,)\n"
     ]
    }
   ],
   "source": [
    "et.interleave(\n",
    "        lambda filepath: tf.data.from_tensor_slices(filepath).skip(23),\n",
    "        cycle_length=1, num_parallel_calls=1)\n",
    "# read data path\n",
    "dryrun_1_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/\"\n",
    "dryrun_1_data = data_path(dryrun_1_path)\n",
    "dryrun_2_path = \"/media/yukun/2C167E66167E30C6/AEdata/20200213_DryRun/water_on_fan/DryRun_2/\"\n",
    "dryrun_2_data = data_path(dryrun_2_path)\n",
    "dryrun_3_path = \"/media/yukun/2C167E66167E30C6/AEdata/20200213_DryRun/water_on_wheel/DryRun_3\"\n",
    "dryrun_3_data = data_path(dryrun_3_path)\n",
    "\n",
    "row0001_1_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190104_1602_row0001_feed200um_1_har1/row0001_1/\"\n",
    "row0001_1_data = data_path(row0001_1_path)\n",
    "row0001_2_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190104_1630_row0001_feed200um_2_har1/row0001_2/\"\n",
    "row0001_2_data = data_path(row0001_2_path)\n",
    "row0001_3_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190104_1720_row0001_feed200um_3_har1/row0001_3/\"\n",
    "row0001_3_data = data_path(row0001_3_path)\n",
    "\n",
    "row005_1_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181203_1404_row005_feed300um_1_nor1/row005_1/\"\n",
    "row005_1_data = data_path(row005_1_path)\n",
    "row005_2_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181204_0900_row005_feed300um_2_nor1/row005_2/\"\n",
    "row005_2_data = data_path(row005_2_path)\n",
    "row005_3_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181207_1550_row005_feed100um_3_nor1/row005_3/\"\n",
    "row005_3_data = data_path(row005_3_path)\n",
    "\n",
    "row01_1_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181225_0234_row01_feed200um_1_sof3/row01_1/\"\n",
    "row01_1_data = data_path(row01_1_path)\n",
    "row01_2_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181225_0310_row01_feed200um_2_sof3/row01_2/\"\n",
    "row01_2_data = data_path(row01_2_path)\n",
    "row01_3_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181225_0350_row01_feed150um_3_sof3/row01_3/\"\n",
    "row01_3_data = data_path(row01_3_path)\n",
    "\n",
    "row1_1_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181227_1010_row1_feed200um_1_sof2/row1_1/\"\n",
    "row1_1_data = data_path(row1_1_path)\n",
    "row1_2_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181227_1102_row1_feed200um_2_sof2/row1_2/\"\n",
    "row1_2_data = data_path(row1_2_path)\n",
    "row1_3_path = \"/media/yukun/2C167E66167E30C6/AEdata/20181227_1145_row1_feed200um_3_sof2/row1_3/\"\n",
    "row1_3_data = data_path(row1_3_path)\n",
    "\n",
    "row3_1_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190104_1355_row3_feed200um_1_sof1/row3_1/\"\n",
    "row3_1_data = data_path(row3_1_path)\n",
    "row3_2_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190104_1430_row3_feed200um_2_sof1/row3_2/\"\n",
    "row3_2_data = data_path(row3_2_path)\n",
    "row3_3_path = \"/media/yukun/2C167E66167E30C6/AEdata/20190104_1510_row3_feed200um_3_sof1/row3_3/\"\n",
    "row3_3_data = data_path(row3_3_path)\n",
    "\n",
    "print('dryrun_1_data :',shape(dryrun_1_data))\n",
    "print('dryrun_2_data :',shape(dryrun_2_data))\n",
    "print('dryrun_3_data :',shape(dryrun_3_data))\n",
    "\n",
    "print('row0001_1_data :',shape(row0001_1_data))\n",
    "print('row0001_2_data :',shape(row0001_2_data))\n",
    "print('row0001_3_data :',shape(row0001_3_data))\n",
    "\n",
    "print('row005_1_data :',shape(row005_1_data))\n",
    "print('row005_2_data :',shape(row005_2_data))\n",
    "print('row005_3_data :',shape(row005_3_data))\n",
    "\n",
    "print('row01_1_data :',shape(row01_1_data))\n",
    "print('row01_2_data :',shape(row01_2_data))\n",
    "print('row01_3_data :',shape(row01_3_data))\n",
    "\n",
    "print('row1_1_data :',shape(row1_1_data))\n",
    "print('row1_2_data :',shape(row1_2_data))\n",
    "print('row1_3_data :',shape(row1_3_data))\n",
    "\n",
    "print('row3_1_data :',shape(row3_1_data))\n",
    "print('row3_2_data :',shape(row3_2_data))\n",
    "print('row3_3_data :',shape(row3_3_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dryrun shape:   (431,)\n",
      "row0001 shape:  (522,)\n",
      "row005 shape:   (502,)\n",
      "row01 shape:    (552,)\n",
      "row1 shape:     (601,)\n",
      "row3 shape:     (603,)\n"
     ]
    }
   ],
   "source": [
    "dryrun_all = np.hstack((dryrun_1_data, dryrun_2_data, dryrun_3_data))\n",
    "row0001_all = np.hstack((row0001_1_data, row0001_2_data, row0001_3_data))\n",
    "row005_all = np.hstack((row005_1_data, row005_2_data, row005_3_data))\n",
    "row01_all = np.hstack((row01_1_data, row01_2_data, row01_3_data))\n",
    "row1_all = np.hstack((row1_1_data, row1_2_data, row1_3_data))\n",
    "row3_all = np.hstack((row3_1_data, row3_2_data, row3_3_data))\n",
    "print('dryrun shape:  ',dryrun_all.shape)\n",
    "print('row0001 shape: ',row0001_all.shape)\n",
    "print('row005 shape:  ',row005_all.shape)\n",
    "print('row01 shape:   ',row01_all.shape)\n",
    "print('row1 shape:    ',row1_all.shape)\n",
    "print('row3 shape:    ',row3_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ae data:  (30,)\n",
      "all label:  (30,)\n"
     ]
    }
   ],
   "source": [
    "ae_data_path = np.hstack((dryrun_all[0:5], row0001_all[0:5], row005_all[0:5], row01_all[0:5], row1_all[0:5], row3_all[0:5]))\n",
    "print('all ae data: ',shape(ae_data_path))\n",
    "\n",
    "# get label\n",
    "ae_label=[]\n",
    "files = [dryrun_all[0:5], row0001_all[0:5], row005_all[0:5], row01_all[0:5], row1_all[0:5], row3_all[0:5]]\n",
    "for index in range(len(files)): \n",
    "    file_dir = files[index]   \n",
    "    col = np.ones([len(file_dir),])\n",
    "    ae_label.extend(col*(index))\n",
    "    \n",
    "ae_label = np.array(ae_label)\n",
    "print('all label: ',shape(ae_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae_val_path = np.hstack((dryrun_all[6:10], row0001_all[6:10], row005_all[6:10], row01_all[6:10], row1_all[6:10], row3_all[6:10]))\n",
    "# print('all ae data: ',shape(ae_val_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preporcess data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file):\n",
    "    class_name = []\n",
    "    for f in file:\n",
    "        if f.find(\"DryRun\") >=0: class_name.append(0)\n",
    "        elif f.find(\"row0001\") >=0: class_name.append(1)\n",
    "        elif f.find(\"row005\") >=0: class_name.append(2)\n",
    "        elif f.find(\"row01\") >=0: class_name.append(3)\n",
    "        elif f.find(\"row1\") >=0: class_name.append(4)\n",
    "        elif f.find(\"row3\") >=0: class_name.append(5)\n",
    "    return tf.one_hot(class_name, 6, dtype=tf.int32)\n",
    "\n",
    "\n",
    "def _read_py_function(filename, label):\n",
    "#     labels = get_label((filename.numpy()).decode('utf-8'))\n",
    "    labels = get_label(filename)\n",
    "#     dt_loading = pd.read_fwf((filename.numpy()).decode('utf-8'), widths=[10])[21:5000021]\n",
    "    dt_loading = pd.read_fwf(filename, widths=[10])[21:5000021]\n",
    "    dt_tartype = np.array(dt_loading, dtype = np.float32)\n",
    "    ae_rawdata = dt_tartype.ravel()\n",
    "    ae_data = ae_rawdata - ae_rawdata.mean()\n",
    "    stfts =tf.signal.stft(ae_data[:], 4096, 2048, fft_length=4096,\n",
    "        window_fn=tf.signal.hann_window, pad_end=False, name=None\n",
    "        )\n",
    "    \n",
    "    magnitude_spectrograms = tf.abs(stfts)\n",
    "    mag_stft = tf.transpose(magnitude_spectrograms[:,:1024])\n",
    "    mag_stft2 = tf.expand_dims(mag_stft, -1)\n",
    "    return mag_stft2, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_function(ae_data, label):    \n",
    "    stfts =tf.signal.stft(ae_data[:], 4096, 2048, fft_length=4096,\n",
    "        window_fn=tf.signal.hann_window, pad_end=False, name=None\n",
    "        )\n",
    "    \n",
    "    magnitude_spectrograms = tf.abs(stfts)\n",
    "    mag_stft = tf.transpose(magnitude_spectrograms[:,:1024])\n",
    "    mag_stft2 = tf.expand_dims(mag_stft, -1)\n",
    "#     spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\n",
    "#     mag_stft2 = tf.reshape(mag_stft, [1024, 2440, 3])    \n",
    "    return mag_stft2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00407372  0.00651471  0.00895672 ... -0.00080928  0.00163172\n",
      " -0.00813328]\n",
      "(5000000,)\n",
      "tf.Tensor([1 0 0 0 0 0], shape=(6,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# data1, label = _read_py_function(ae_data_path[0])\n",
    "print(data1)\n",
    "print(data1.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.2195969 ]\n",
      "  [2.2187958 ]\n",
      "  [2.2669706 ]\n",
      "  ...\n",
      "  [0.7109188 ]\n",
      "  [1.8340455 ]\n",
      "  [1.4269618 ]]\n",
      "\n",
      " [[0.57142705]\n",
      "  [1.089855  ]\n",
      "  [1.2435471 ]\n",
      "  ...\n",
      "  [0.9288089 ]\n",
      "  [0.6687688 ]\n",
      "  [0.50131255]]\n",
      "\n",
      " [[0.5298968 ]\n",
      "  [0.10556262]\n",
      "  [0.38851884]\n",
      "  ...\n",
      "  [0.48943955]\n",
      "  [0.7725617 ]\n",
      "  [0.22039677]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.13043717]\n",
      "  [0.4092054 ]\n",
      "  [0.09629249]\n",
      "  ...\n",
      "  [0.13149974]\n",
      "  [0.53997076]\n",
      "  [0.10595431]]\n",
      "\n",
      " [[0.49097404]\n",
      "  [0.39089718]\n",
      "  [0.15794124]\n",
      "  ...\n",
      "  [0.37124333]\n",
      "  [0.18797149]\n",
      "  [0.65650445]]\n",
      "\n",
      " [[0.6753486 ]\n",
      "  [0.4377786 ]\n",
      "  [0.5258601 ]\n",
      "  ...\n",
      "  [0.33195838]\n",
      "  [0.34853783]\n",
      "  [0.49736   ]]], shape=(1024, 2440, 1), dtype=float32)\n",
      "tf.Tensor([1 0 0 0 0 0], shape=(6,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "mag_stft2, label2 = _preprocess_function(data1, label)\n",
    "print(mag_stft2)\n",
    "print(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_32.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_33.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_34.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_35.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_36.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1602_row0001_feed200um_1_har1/row0001_1/AEdate_629.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1602_row0001_feed200um_1_har1/row0001_1/AEdate_630.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1602_row0001_feed200um_1_har1/row0001_1/AEdate_631.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1602_row0001_feed200um_1_har1/row0001_1/AEdate_632.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1602_row0001_feed200um_1_har1/row0001_1/AEdate_633.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181203_1404_row005_feed300um_1_nor1/row005_1/AEdate_62.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181203_1404_row005_feed300um_1_nor1/row005_1/AEdate_63.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181203_1404_row005_feed300um_1_nor1/row005_1/AEdate_64.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181203_1404_row005_feed300um_1_nor1/row005_1/AEdate_65.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181203_1404_row005_feed300um_1_nor1/row005_1/AEdate_66.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181225_0234_row01_feed200um_1_sof3/row01_1/AEdate_71.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181225_0234_row01_feed200um_1_sof3/row01_1/AEdate_72.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181225_0234_row01_feed200um_1_sof3/row01_1/AEdate_73.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181225_0234_row01_feed200um_1_sof3/row01_1/AEdate_74.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181225_0234_row01_feed200um_1_sof3/row01_1/AEdate_75.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181227_1010_row1_feed200um_1_sof2/row1_1/AEdate_1.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181227_1010_row1_feed200um_1_sof2/row1_1/AEdate_2.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181227_1010_row1_feed200um_1_sof2/row1_1/AEdate_3.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181227_1010_row1_feed200um_1_sof2/row1_1/AEdate_4.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20181227_1010_row1_feed200um_1_sof2/row1_1/AEdate_5.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1355_row3_feed200um_1_sof1/row3_1/AEdate_8.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1355_row3_feed200um_1_sof1/row3_1/AEdate_9.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1355_row3_feed200um_1_sof1/row3_1/AEdate_10.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1355_row3_feed200um_1_sof1/row3_1/AEdate_11.lvm'\n",
      " b'/media/yukun/2C167E66167E30C6/AEdata/20190104_1355_row3_feed200um_1_sof1/row3_1/AEdate_12.lvm'], shape=(30,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]], shape=(30, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# x_data = tf.data.Dataset.from_tensor_slices(tf.constant(ae_data_path))\n",
    "x_data = tf.constant(ae_data_path)\n",
    "print(x_data)\n",
    "\n",
    "# y_class = ['dryrun', 'row0001', 'row005', 'row01', 'row1', 'row3']\n",
    "y_class = get_label(ae_data_path)\n",
    "# y_data = tf.data.Dataset.from_tensor_slices(tf.constant(y_class))\n",
    "y_data = tf.constant(y_class)\n",
    "print(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipDataset shapes: {x: (), y: ()}, types: {x: tf.string, y: tf.int32}>\n"
     ]
    }
   ],
   "source": [
    "# dataset = tf.data.Dataset.zip({'x':x_data, 'y':y_data})\n",
    "# print(dataset)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_32.lvm', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(b'/media/yukun/2C167E66167E30C6/AEdata/20190313_1113_DryRun/DryRun_1/AEdate_33.lvm', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for data in dataset.take(2):\n",
    "    print(data['x'])\n",
    "    print(data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_tensor_slices() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5de641a816e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: from_tensor_slices() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
